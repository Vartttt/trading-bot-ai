# ==============================
# ‚úÖ UNIVERSAL IMPORT & MODEL_DIR HANDLER
# ==============================
import os
import sys

current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.abspath(os.path.join(current_dir, ".."))
if root_dir not in sys.path:
    sys.path.append(root_dir)

try:
    from config.config import MODEL_DIR
except ModuleNotFoundError:
    print("‚ö†Ô∏è –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –º–æ–¥—É–ª—å 'config'. –î–æ–¥–∞—é –≤—Ä—É—á–Ω—É...")
    sys.path.append(os.path.join(root_dir, "config"))
    from config import config
    MODEL_DIR = getattr(config, "MODEL_DIR", os.path.join(root_dir, "models"))
    print("‚úÖ MODEL_DIR —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ –≤—Ä—É—á–Ω—É.")

os.makedirs(MODEL_DIR, exist_ok=True)
print(f"‚úÖ MODEL_DIR –∞–∫—Ç–∏–≤–Ω–∏–π —à–ª—è—Ö: {MODEL_DIR}")

# --- –¥–µ—Ñ–æ–ª—Ç–Ω—ñ —Ñ—ñ—á—ñ ---
DEFAULT_FEATURE_COLS = ["ema_diff5", "rsi5", "atr", "volz5", "trend_accel"]
TARGET_COLS = ["next_return", "target"]

# ==============================
# üîö END OF UNIVERSAL IMPORT FIX
# ==============================
import json
import time
import traceback
import requests
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import ta
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import StandardScaler
from joblib import dump, load
from notifier.telegram_notifier import send_message

def _resolve_feature_cols(df: pd.DataFrame):
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫-–æ–∑–Ω–∞–∫, —è–∫—ñ —Ä–µ–∞–ª—å–Ω–æ –ø—Ä–∏—Å—É—Ç–Ω—ñ —É df.
    1) –Ω–∞–º–∞–≥–∞—î–º–æ—Å—å –≤–∑—è—Ç–∏ –ø–µ—Ä–µ—Ç–∏–Ω —ñ–∑ DEFAULT_FEATURE_COLS;
    2) —è–∫—â–æ –Ω—ñ—á–æ–≥–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ ‚Äî –±–µ—Ä–µ–º–æ –≤—Å—ñ —á–∏—Å–ª–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏,
       –∫—Ä—ñ–º —Ü—ñ–ª—å–æ–≤–∏—Ö —ñ —Å–∏—Ä–∏—Ö OHLCV/—á–∞—Å—É.
    """
    # 1) –ø–µ—Ä–µ—Ç–∏–Ω —ñ–∑ –¥–µ—Ñ–æ–ª—Ç–Ω–∏–º —Å–ø–∏—Å–∫–æ–º
    cols = [c for c in DEFAULT_FEATURE_COLS if c in df.columns]

    # 2) —Ñ–æ–ª–±–µ–∫ ‚Äî –≤—Å—ñ —á–∏—Å–ª–æ–≤—ñ, –∫—Ä—ñ–º –∑–∞–±–æ—Ä–æ–Ω–µ–Ω–∏—Ö
    if not cols:
        blacklist = set(TARGET_COLS + ["open_time", "open", "high", "low", "close", "volume"])
        cols = [c for c in df.columns
                if c not in blacklist and pd.api.types.is_numeric_dtype(df[c])]

    if not cols:
        raise ValueError("–ù–µ –≤–¥–∞–ª–æ—Å—è –≤–∏–∑–Ω–∞—á–∏—Ç–∏ feature_cols ‚Äî —É DataFrame –Ω–µ–º–∞—î –ø—Ä–∏–¥–∞—Ç–Ω–∏—Ö –æ–∑–Ω–∞–∫.")
    return cols

# ============================================================
# ‚öôÔ∏è –®–ª—è—Ö–∏
# ============================================================
MODEL_PATH = os.path.join(MODEL_DIR, "transformer_signal_model.pt")
SCALER_PATH = os.path.join(MODEL_DIR, "transformer_scaler.joblib")
TRAIN_DATA_PATH = os.path.join(MODEL_DIR, "train_data.json")
FLAG_PATH = "/tmp/last_auto_retrain.txt"
FEATURE_COLS_PATH = os.path.join(MODEL_DIR, "feature_cols.json")

# ============================================================
# ‚öôÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —ñ—Å—Ç–æ—Ä–∏—á–Ω–∏—Ö —Å–≤—ñ—á–æ–∫
# ============================================================
def load_training_data(symbol="BTCUSDT", interval="15m", limit=20000):
    print(f"üìä –ó–∞–≤–∞–Ω—Ç–∞–∂—É—é {limit} —Å–≤—ñ—á–æ–∫ –∑ MEXC –¥–ª—è {symbol} ({interval})...")
    url = f"https://api.mexc.com/api/v3/klines?symbol={symbol}&interval={interval}&limit={limit}"

    try:
        r = requests.get(url, timeout=15)
        data = r.json()

        if not isinstance(data, list):
            print("‚ùå –ù–µ–∫–æ—Ä–µ–∫—Ç–Ω–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å API MEXC:", data)
            return []

        df = pd.DataFrame(data, columns=[
            "open_time", "open", "high", "low", "close", "volume",
            "close_time", "quote_asset_volume", "trades", "taker_base",
            "taker_quote", "ignore"
        ])

        df = df.astype({
            "open": float, "high": float, "low": float, "close": float, "volume": float
        })
        df.dropna(inplace=True)

        # --- –Ü–Ω–¥–∏–∫–∞—Ç–æ—Ä–∏ ---
        df["ema9"] = ta.trend.EMAIndicator(df["close"], 9).ema_indicator()
        df["ema21"] = ta.trend.EMAIndicator(df["close"], 21).ema_indicator()
        df["ema_diff5"] = df["ema9"] - df["ema21"]

        df["rsi5"] = ta.momentum.RSIIndicator(df["close"], 14).rsi()
        df["atr"] = ta.volatility.AverageTrueRange(df["high"], df["low"], df["close"], 14).average_true_range()
        df["volz5"] = (df["volume"] - df["volume"].rolling(20).mean()) / (df["volume"].rolling(20).std() + 1e-9)

        # üß† –ù–û–í–ê –§–Ü–ß–ê ‚Äî –ø—Ä–∏—Å–∫–æ—Ä–µ–Ω–Ω—è —Ç—Ä–µ–Ω–¥—É
        df["trend_accel"] = df["ema_diff5"].diff()

        df.dropna(inplace=True)

        # –û—Å—Ç–∞–Ω–Ω—ñ 20 000 —Ä—è–¥–∫—ñ–≤
        df = df.tail(limit)

        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —É JSON
        df_out = df[["ema_diff5", "rsi5", "atr", "volz5", "trend_accel"]].to_dict(orient="records")

        os.makedirs(MODEL_DIR, exist_ok=True)
        with open(TRAIN_DATA_PATH, "w", encoding="utf-8") as f:
            json.dump(df_out, f, ensure_ascii=False, indent=2)

        print(f"‚úÖ –î–∞–Ω—ñ –∑–±–µ—Ä–µ–∂–µ–Ω–æ: models/train_data.json ({len(df_out)} —Ä—è–¥–∫—ñ–≤)")
        return df_out

    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—ñ —ñ—Å—Ç–æ—Ä—ñ—ó: {e}")
        return []

def ensure_artifacts():
    os.makedirs(MODEL_DIR, exist_ok=True)

    # —è–∫—â–æ feature_cols.json –≤—ñ–¥—Å—É—Ç–Ω—ñ–π ‚Äî —Å—Ç–≤–æ—Ä—é—î–º–æ –∑ –¥–µ—Ñ–æ–ª—Ç–Ω–∏–º —Å–ø–∏—Å–∫–æ–º
    if not os.path.exists(FEATURE_COLS_PATH):
        with open(FEATURE_COLS_PATH, "w", encoding="utf-8") as f:
            json.dump(DEFAULT_FEATURE_COLS, f, ensure_ascii=False, indent=2)
        print(f"üÜï –°—Ç–≤–æ—Ä–µ–Ω–æ {FEATURE_COLS_PATH} (–¥–µ—Ñ–æ–ª—Ç–Ω—ñ —Ñ—ñ—á—ñ).")
    else:
        print(f"‚úÖ {os.path.basename(FEATURE_COLS_PATH)} —ñ—Å–Ω—É—î.")

# ============================================================
# üß† Dataset
# ============================================================
class SignalDataset(Dataset):
    def __init__(self, data, seq_len=50):
        X, y = [], []
        for i in range(len(data) - seq_len):
            seq = data[i:i + seq_len, :-1]
            target = data[i + seq_len, -1]
            X.append(seq)
            y.append(target)
        self.X = np.array(X, dtype=np.float32)
        self.y = np.array(y, dtype=np.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

# ============================================================
# üß© Transformer Model
# ============================================================
class SignalTransformer(nn.Module):
    def __init__(self, input_dim, embed_dim=64, n_heads=4, ff_dim=128, num_layers=2):
        super().__init__()
        self.embedding = nn.Linear(input_dim, embed_dim)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, nhead=n_heads, dim_feedforward=ff_dim,
            dropout=0.1, batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Sequential(
            nn.Linear(embed_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.embedding(x)
        encoded = self.encoder(x)
        out = encoded[:, -1, :]
        return self.fc(out)

# ============================================================
# üèãÔ∏è‚Äç‚ôÇÔ∏è Train / Save  (–≤–∏–ø—Ä–∞–≤–ª–µ–Ω–æ)
# ============================================================
def train_transformer(epochs=20, batch_size=32, seq_len=50):
    try:
        ensure_artifacts()

        if not os.path.exists(TRAIN_DATA_PATH):
            print("‚ö†Ô∏è –ù–µ–º–∞—î train_data.json ‚Äî —Å—Ç–≤–æ—Ä—é—é...")
            load_training_data(limit=20000)

        df = pd.read_json(TRAIN_DATA_PATH, orient="records")
        if df.empty:
            raise ValueError("train_data.json –ø–æ—Ä–æ–∂–Ω—ñ–π!")

        # –≤–∏–∑–Ω–∞—á–∞—î–º–æ —Ç–∞ –∑–±–µ—Ä—ñ–≥–∞—î–º–æ –ø–æ—Ä—è–¥–æ–∫ —Ñ—ñ—á
        feature_cols = _resolve_feature_cols(df)
        with open(FEATURE_COLS_PATH, "w", encoding="utf-8") as f:
            json.dump(feature_cols, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ –í–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ —Ñ—ñ—á—ñ: {feature_cols}")

        # X ‚Äî —Ç—ñ–ª—å–∫–∏ —Ñ—ñ—á—ñ, y ‚Äî –æ–∫—Ä–µ–º–æ (—Ç–∏–º—á–∞—Å–æ–≤–∏–π —Ç–∞—Ä–≥–µ—Ç-–∑–∞–≥–ª—É—à–∫–∞)
        X = df[feature_cols].fillna(0).values
        y = np.random.uniform(0, 1, len(df)).reshape(-1, 1)

        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        dump(scaler, SCALER_PATH)

        # –º–∞—Ç—Ä–∏—Ü—è –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç—É: –æ—Å—Ç–∞–Ω–Ω—è –∫–æ–ª–æ–Ω–∫–∞ ‚Äî —Ü–µ —Ç–∞—Ä–≥–µ—Ç
        data_mat = np.hstack([X_scaled, y])

        dataset = SignalDataset(data_mat, seq_len)
        loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        model = SignalTransformer(input_dim=len(feature_cols))
        criterion = nn.BCELoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

        for epoch in range(epochs):
            total_loss = 0.0
            for Xb, yb in loader:
                optimizer.zero_grad()
                preds = model(Xb).squeeze()
                loss = criterion(preds, yb)
                loss.backward()
                optimizer.step()
                total_loss += loss.item()
            print(f"üß† Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(loader):.6f}")

        torch.save(model.state_dict(), MODEL_PATH)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ: {MODEL_PATH}")
        send_message("ü§ñ –ú–æ–¥–µ–ª—å —É—Å–ø—ñ—à–Ω–æ –ø–µ—Ä–µ–≤—á–µ–Ω–∞ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–∞! ‚úÖ")

        with open(FLAG_PATH, "w") as f:
            f.write(str(time.time()))

    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {e}")
        traceback.print_exc()
        send_message(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—ñ –º–æ–¥–µ–ª—ñ: {e}")

# ============================================================
# üîÆ Predict  (—Ä–µ–∞–ª—å–Ω–∏–π —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å)
# ============================================================
def predict_strength(feature_rows, seq_len=50):
    """
    feature_rows: —Å–ø–∏—Å–æ–∫ dict'—ñ–≤ –∞–±–æ DataFrame –∑ —Ç–∏–º–∏ —Å–∞–º–∏–º–∏ —Ñ—ñ—á–∞–º–∏, —â–æ –π –ø—ñ–¥ —á–∞—Å —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è.
                  –ú–∞—î –º—ñ—Å—Ç–∏—Ç–∏ –ø—Ä–∏–Ω–∞–π–º–Ω—ñ `seq_len` –æ—Å—Ç–∞–Ω–Ω—ñ—Ö —Ä—è–¥–∫—ñ–≤ (–±—É–¥–µ –ø–∞–¥–¥—ñ–Ω–≥, —è–∫—â–æ –º–µ–Ω—à–µ).
    –ü–æ–≤–µ—Ä—Ç–∞—î: –≤—ñ–¥—Å–æ—Ç–æ–∫ —Å–∏–ª–∏ —Å–∏–≥–Ω–∞–ª—É (0..100).
    """
    try:
        ensure_artifacts()

        # 1) –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –ø–æ—Ä—è–¥–æ–∫ —Ñ—ñ—á
        if os.path.exists(FEATURE_COLS_PATH):
            with open(FEATURE_COLS_PATH, "r", encoding="utf-8") as f:
                feature_cols = json.load(f)
        else:
            feature_cols = DEFAULT_FEATURE_COLS
        if not feature_cols:
            raise ValueError("feature_cols.json –ø–æ—Ä–æ–∂–Ω—ñ–π ‚Äî –Ω–µ–º–∞—î —Å–ø–∏—Å–∫—É —Ñ—ñ—á.")

        # 2) –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ scaler —ñ –º–æ–¥–µ–ª—å
        scaler = load(SCALER_PATH)
        model = SignalTransformer(input_dim=len(feature_cols))
        model.load_state_dict(torch.load(MODEL_PATH, map_location="cpu"))
        model.eval()

        # 3) –ì–æ—Ç—É—î–º–æ –¥–∞–Ω—ñ
        if isinstance(feature_rows, pd.DataFrame):
            df_infer = feature_rows.copy()
        else:
            df_infer = pd.DataFrame(feature_rows)

        X = df_infer[feature_cols].fillna(0).values
        X_scaled = scaler.transform(X)

        # –ü–∞–¥–¥—ñ–Ω–≥ –¥–æ seq_len (–ø–æ–≤—Ç–æ—Ä—é—î–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Ä—è–¥–æ–∫)
        if X_scaled.shape[0] < seq_len:
            if X_scaled.shape[0] == 0:
                raise ValueError("–ù–µ–º–∞—î –∂–æ–¥–Ω–æ–≥–æ —Ä—è–¥–∫–∞ –¥–ª—è —ñ–Ω—Ñ–µ—Ä–µ–Ω—Å—É.")
            pad = np.repeat(X_scaled[-1:], seq_len - X_scaled.shape[0], axis=0)
            X_scaled = np.vstack([X_scaled, pad])

        # 4) –Ü–Ω—Ñ–µ—Ä–µ–Ω—Å
        x_seq = torch.tensor(X_scaled[-seq_len:], dtype=torch.float32).unsqueeze(0)  # (1, seq_len, input_dim)
        with torch.no_grad():
            pred = float(model(x_seq).squeeze().item())

        pred_pct = max(0.0, min(1.0, pred)) * 100.0
        send_message(f"üìä –°–∏–ª–∞ —Å–∏–≥–Ω–∞–ª—É –®–Ü: {pred_pct:.2f}%")
        return round(pred_pct, 2)

    except Exception as e:
        print(f"‚ö†Ô∏è predict_strength error: {e}")
        traceback.print_exc()
        send_message(f"‚ö†Ô∏è –ü–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –ø—Ä–æ–≥–Ω–æ–∑—É —Å–∏–ª–∏ —Å–∏–≥–Ω–∞–ª—É: {e}")
        return 50.0

# ============================================================
# üöÄ Main
# ============================================================
if __name__ == "__main__":
    send_message("üöÄ AI Transformer –∑–∞–ø—É—â–µ–Ω–∏–π. –ü–æ—á–∏–Ω–∞—é —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    if not os.path.exists(TRAIN_DATA_PATH):
        load_training_data(limit=20000)
    else:
        mtime = os.path.getmtime(TRAIN_DATA_PATH)
        if (time.time() - mtime) / 3600 > 24:
            print("üîÅ –û–Ω–æ–≤–ª—é—é train_data.json...")
            load_training_data(limit=20000)

    train_transformer(epochs=20, seq_len=10)
    send_message("‚úÖ –ù–∞–≤—á–∞–Ω–Ω—è –∑–∞–≤–µ—Ä—à–µ–Ω–æ, –º–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞ –¥–æ —Ä–æ–±–æ—Ç–∏!")






