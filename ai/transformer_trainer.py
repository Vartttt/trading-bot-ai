# ==============================
# ‚úÖ UNIVERSAL IMPORT & MODEL_DIR HANDLER
# ==============================
import os
import sys

# –ü–æ—Ç–æ—á–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è —Ñ–∞–π–ª—É (ai/)
current_dir = os.path.dirname(os.path.abspath(__file__))

# –ö–æ—Ä–µ–Ω–µ–≤–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—è –ø—Ä–æ—î–∫—Ç—É (/workspaces/)
root_dir = os.path.abspath(os.path.join(current_dir, ".."))

# –î–æ–¥–∞—î–º–æ root —É sys.path
if root_dir not in sys.path:
    sys.path.append(root_dir)

# –Ü–º–ø–æ—Ä—Ç –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó (config/config.py)
try:
    from config.config import MODEL_DIR
except ModuleNotFoundError:
    print("‚ö†Ô∏è  –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ –º–æ–¥—É–ª—å 'config'. –°–ø—Ä–æ–±—É—é –¥–æ–¥–∞—Ç–∏ —à–ª—è—Ö –≤—Ä—É—á–Ω—É...")
    sys.path.append(os.path.join(root_dir, "config"))
    from config import config
    MODEL_DIR = getattr(config, "MODEL_DIR", os.path.join(root_dir, "models"))
    print("‚úÖ MODEL_DIR —ñ–º–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ –ø—ñ—Å–ª—è —Ä—É—á–Ω–æ–≥–æ –¥–æ–¥–∞–≤–∞–Ω–Ω—è —à–ª—è—Ö—É.")

# –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —ñ—Å–Ω—É–≤–∞–Ω–Ω—è MODEL_DIR
os.makedirs(MODEL_DIR, exist_ok=True)
print(f"‚úÖ MODEL_DIR –∞–∫—Ç–∏–≤–Ω–∏–π —à–ª—è—Ö: {MODEL_DIR}")

# ==============================
# üîö END OF UNIVERSAL IMPORT FIX
# ==============================

import json
import time
import traceback
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import StandardScaler
from joblib import dump, load

MODEL_PATH = os.path.join(MODEL_DIR, "transformer_signal_model.pt")
SCALER_PATH = os.path.join(MODEL_DIR, "transformer_scaler.joblib")
TRAIN_DATA_PATH = os.path.join(MODEL_DIR, "train_data.json")

# –¥–ª—è –∞–≤—Ç–æ–ø–µ—Ä–µ–≤—á–∞–Ω–Ω—è
COOLDOWN_SEC = int(os.getenv("RETRAIN_COOLDOWN_SEC", 6 * 60 * 60))
FLAG_PATH = "/tmp/last_auto_retrain.txt"

# –∫–∞–Ω–æ–Ω—ñ—á–Ω–∏–π —Å–ø–∏—Å–æ–∫ —Ñ—ñ—á (—Ç—ñ, —â–æ –π—à–ª–∏ —É —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –±–µ–∑ —Ç–∞—Ä–≥–µ—Ç—É)
FEATURE_COLS = ["ema_diff5", "rsi5", "atr", "volz5"]
TARGET_COL = "strength"


# ============================================================
# üìò Dataset Definition
# ============================================================
class SignalDataset(Dataset):
    def __init__(self, data, seq_len=50):
        X, y = [], []
        for i in range(len(data) - seq_len):
            seq = data[i:i + seq_len, :-1]
            target = data[i + seq_len, -1]
            X.append(seq)
            y.append(target)
        self.X = np.array(X, dtype=np.float32)
        self.y = np.array(y, dtype=np.float32)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


# ============================================================
# üß† Transformer Model
# ============================================================
class SignalTransformer(nn.Module):
    def __init__(self, input_dim, embed_dim=64, n_heads=4, ff_dim=128, num_layers=2):
        super().__init__()
        self.embedding = nn.Linear(input_dim, embed_dim)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=n_heads,
            dim_feedforward=ff_dim,
            dropout=0.1,
            batch_first=True
        )

        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Sequential(
            nn.Linear(embed_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.embedding(x)           # (batch, seq, embed_dim)
        x = x.permute(1, 0, 2)          # (seq, batch, embed_dim)
        encoded = self.encoder(x)
        out = encoded[-1]               # (batch, embed_dim) ‚Äî –æ—Å—Ç–∞–Ω–Ω—ñ–π –∫—Ä–æ–∫
        return self.fc(out)             # (batch, 1)


# ============================================================
# ‚öôÔ∏è Train / Save
# ============================================================
def train_transformer(epochs=10, batch_size=32, seq_len=50):

    if not os.path.exists(TRAIN_DATA_PATH):
        print("‚ö†Ô∏è –ù–µ–º–∞—î train_data.json ‚Äî —Å–ø–æ—á–∞—Ç–∫—É –∑–≥–µ–Ω–µ—Ä—É–π —ñ—Å—Ç–æ—Ä—ñ—é —Å–∏–≥–Ω–∞–ª—ñ–≤.")
        return

    with open(TRAIN_DATA_PATH, "r") as f:
        data = json.load(f)

    if not isinstance(data, list) or len(data) == 0:
        print("‚ùå train_data.json –ø–æ—Ä–æ–∂–Ω—ñ–π –∞–±–æ –Ω–µ–∫–æ—Ä–µ–∫—Ç–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç!")
        return

    df = pd.DataFrame(data)
    print(f"üßæ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ DataFrame: {df.shape}")
    print("üîë –ö–æ–ª–æ–Ω–∫–∏:", df.columns.tolist())
    print(df.head(3))

    # üîß –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è strength —É –¥—ñ–∞–ø–∞–∑–æ–Ω [0, 1]
    if df[TARGET_COL].max() > 1 or df[TARGET_COL].min() < 0:
        print("‚öôÔ∏è Strength –≤–∏—Ö–æ–¥–∏—Ç—å –∑–∞ –º–µ–∂—ñ [0,1], –≤–∏–∫–æ–Ω—É—î–º–æ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—é...")
        df[TARGET_COL] = (df[TARGET_COL] - df[TARGET_COL].min()) / (df[TARGET_COL].max() - df[TARGET_COL].min())
    df[TARGET_COL] = df[TARGET_COL].clip(0, 1)

    # –∑–∞–ª–∏—à–∞—î–º–æ –ª–∏—à–µ –ø–æ—Ç—Ä—ñ–±–Ω—ñ –∫–æ–ª–æ–Ω–∫–∏
    df = df[FEATURE_COLS + [TARGET_COL]].fillna(0)
    print(f"üìä –†—è–¥–∫—ñ–≤ –¥–æ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è: {len(df)}")

    # –ú–∞—Å—à—Ç–∞–±—É—î–º–æ –¢–Ü–õ–¨–ö–ò —Ñ—ñ—á—ñ, –Ω–µ —Ç–∞—Ä–≥–µ—Ç
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(df[FEATURE_COLS].values)
    dump(scaler, SCALER_PATH)

    y = df[TARGET_COL].values.reshape(-1, 1)
    data_mat = np.hstack([X_scaled, y])           # –æ—Å—Ç–∞–Ω–Ω—è –∫–æ–ª–æ–Ω–∫–∞ ‚Äî —Ç–∞—Ä–≥–µ—Ç

    dataset = SignalDataset(data_mat, seq_len)
    print(f"üìè –î–æ–≤–∂–∏–Ω–∞ dataset –ø—ñ—Å–ª—è —Ñ–æ—Ä–º—É–≤–∞–Ω–Ω—è: {len(dataset)}")

    if len(dataset) == 0:
        print("‚ö†Ô∏è Dataset –ø–æ—Ä–æ–∂–Ω—ñ–π ‚Äî –∑–±—ñ–ª—å—à—É–π –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Ä—è–¥–∫—ñ–≤ —É train_data.json (–º—ñ–Ω—ñ–º—É–º 60-70).")
        return

    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    model = SignalTransformer(input_dim=len(FEATURE_COLS))
    criterion = nn.BCELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

    for epoch in range(epochs):
        total_loss = 0.0
        for Xb, yb in loader:
            optimizer.zero_grad()
            preds = model(Xb).squeeze()
            loss = criterion(preds, yb)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"üß† Epoch {epoch+1}/{epochs} | Loss: {total_loss/len(loader):.6f}")

    torch.save(model.state_dict(), MODEL_PATH)
    print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ ‚Üí {MODEL_PATH}")


# ============================================================
# üîÆ Predict
# ============================================================
def predict_strength(features_dict):
    try:
        scaler = load(SCALER_PATH)
        model = SignalTransformer(input_dim=len(FEATURE_COLS))
        model.load_state_dict(torch.load(MODEL_PATH, map_location="cpu"))
        model.eval()

        x = np.array([[features_dict[c] for c in FEATURE_COLS]], dtype=float)  # (1, n_features)
        x_scaled = scaler.transform(x)
        x_t = torch.tensor(x_scaled, dtype=torch.float32).unsqueeze(1)         # (1, 1, n_features)

        with torch.no_grad():
            pred = model(x_t).item()
        return float(pred * 100)

    except Exception as e:
        print("‚ö†Ô∏è predict_strength error:", e)

        # ‚öôÔ∏è –∞–≤—Ç–æ–ø–µ—Ä–µ–≤—á–∞–Ω–Ω—è –∑ –∫—É–ª–¥–∞—É–Ω–æ–º
        last = 0.0
        try:
            with open(FLAG_PATH, "r") as f:
                last = float(f.read().strip())
        except Exception:
            pass

        now = time.time()
        if now - last >= COOLDOWN_SEC:
            print("‚ôªÔ∏è –ü–µ—Ä–µ–≤—á–∞—é –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ –∑–º—ñ–Ω—É –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Ñ—ñ—á...")
            try:
                from ai.transformer_trainer import train_transformer
                train_transformer(epochs=10, seq_len=10)
                print("‚úÖ –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ–≤—á–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ!")
                try:
                    with open(FLAG_PATH, "w") as f:
                        f.write(str(now))
                except Exception:
                    pass
                try:
                    from notifier.telegram_bot import send_message
                    send_message("ü§ñ –ú–æ–¥–µ–ª—å –±—É–ª–∞ –ø–µ—Ä–µ–≤—á–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –ø—ñ—Å–ª—è –∑–º—ñ–Ω–∏ —Ñ—ñ—á ‚úÖ")
                except Exception as te:
                    print("‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –Ω–∞–¥—ñ—Å–ª–∞—Ç–∏ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è –≤ Telegram:", te)
            except Exception as retrain_error:
                print("‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∞–≤—Ç–æ–Ω–∞–≤—á–∞–Ω–Ω—ñ:", retrain_error)
                traceback.print_exc()
        else:
            wait = int(COOLDOWN_SEC - (now - last))
            print(f"‚è≥ –ê–≤—Ç–æ–ø–µ—Ä–µ–≤—á–∞–Ω–Ω—è –ø—Ä–æ–ø—É—â–µ–Ω–æ: –∫—É–ª–¥–∞—É–Ω —â–µ {wait}s.")

        # –±–µ–∑–ø–µ—á–Ω–∏–π —Ñ–æ–ª–±–µ–∫
        return 50.0


if __name__ == "__main__":
    train_transformer(epochs=15, seq_len=10)


